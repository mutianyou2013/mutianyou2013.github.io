---
layout: post
read_time: true
show_date: true
title:  Understanding Mechanisms of CO2 Electroreduction on Cu(100) and Cu Surface Evolution with Kinetic Monte Carlo Simulations
date:   2021-03-18 15:14:20 -0600
description: "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?"
img: posts/20210318/TicTacToeSml.jpg
tags: [machine learning, artificial intelligence, reinforcement learning, coding, python]
author: Armando Maynez
github: amaynez/TicTacToe/
toc: yes # leave empty or erase for no TOC
---
## Introduction

Electrochemical reduction of CO2 has attracted researchers’ attention as it has the potential to utilize the abundant greenhouse gas in the Earth’s atmosphere and store intermittent energy from solar panels and wind turbines in chemical bonds. A commercially viable catalyst for CO2 electroreduction should meet the cost-effective requirement while possessing high efficiency and selectivity. Experimental studies showed that Copper (Cu) can catalyze direct electrochemical reduction of CO2 to hydrocarbons in an aqueous bicarbonate solution with a high current density. Some Studies also showed there was a surface evolution of Cu surface under the reducing potential.

<center><img src="./assets/img/posts/20190401/CO2ReductionStatus.png" width="620"></center>

## Challenges

- The underlying mechanism remains elusive due to multistep kinetic pathways.
- Improved kinetic models are needed to consider various complexites of surface reactions at solid/electrolyte interfaces.
- Mechanisms of CO2 reduction can be better studied with more understanding of Cu surface evolution.

## Reaction Network toward C2 species

<center><img src="./assets/img/posts/20190401/pathway.png" width="660"></center>

## C2 Kinetics from Experiment and Modeling

<center><img src="./assets/img/posts/20190401/exp.png" width="660"></center>

<center><img src="./assets/img/posts/20190401/modeling.png" width="600"></center>

- KMC captures the trend of experimental CV curve.- It shows some differences compared to mean field approximation.

## Surface Configuration from KMC

<center><img src="./assets/img/posts/20190401/kmc.png" width="660"></center>

- With applied potential at -0.5 V vs RHE - COads dominates the surface.- The surface has a small amount of Hads and H2Oads.

## Surface Evolution of Cu(100) cube

<center><img src="./assets/img/posts/20190401/evolution.png" width="660"></center>

<center><img src="./assets/img/posts/20190401/site.png" width="420"></center>

<center><img src="./assets/img/posts/20190401/gaussian.png" width="540"></center>

## Conclusions

- By considering adsorbate-adsorbate interactions, KMC model can better understand the mechanism of CO2  reduction.
- By employing artificial neural networks, the diffusion processes can be better simulated to help understanding the evolution of Cu surface.

## References

- [Stamatakis, Michail, and Dionisios G. Vlachos. 2011. “A Graph-Theoretical Kinetic Monte Carlo Framework for on-Lattice Chemical Kinetics.” The Journal of Chemical Physics 134 (21): 214115.](https://aip.scitation.org/doi/10.1063/1.3596751)
- [V Jansson, E Baibuz, and F Djurabekova. “Long-term stability of Cu surface nanotips.” Nanotechnology, 27(26):265708, 2016, arXiv:1508.06870.](https://iopscience.iop.org/article/10.1088/0957-4484/27/26/265708)
- [Goodpaster, J. D., Bell, A. T. & Head-Gordon, M. “Identification of Possible Pathways for C–C Bond Formation during Electrochemical Reduction of CO2: New Theoretical Insights from an Improved Electrochemical Model.” J. Phys. Chem. Lett. 7, 1471–1477 (2016).](https://pubs.acs.org/doi/10.1021/acs.jpclett.6b00358)
- [Peterson, A. A., Abild-Pedersen, F., Studt, F., Rossmeisl, J. & Nørskov, J. K. “How copper catalyzes the electroreduction of carbon dioxide into hydrocarbon fuels.” Energy Environ. Sci. 3, 1311–1315 (2010).](https://pubs.rsc.org/en/content/articlelanding/2010/ee/c0ee00071j)
- [Garza, Alejandro J., Alexis T. Bell, and Martin Head-Gordon. 2018. “Mechanism of CO2 Reduction at Copper Surfaces: Pathways to C2 Products.” ACS Catalysis 8 (2): 1490–99.](https://pubs.acs.org/doi/10.1021/acscatal.7b03477)

## The many models...
### Model 1 - the first try

 [hard coded algorithm](https://github.com/amaynez/TicTacToe/blob/


The resulting learning rate combining the cycles and decay per epoch is:
<center><img src='./assets/img/posts/20210318/LR_cycle_decay.png' width="480">
<small>Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs,<br>
        max Learning Rate factor = 10x</small></center>

```python
true_epoch = epoch - c.BATCH_SIZE
learning_rate = self.learning_rate*(1/(1+c.DECAY_RATE*true_epoch))
if c.CLR_ON: learning_rate = self.cyclic_learning_rate(learning_rate,true_epoch)
```
```python
@staticmethod
def cyclic_learning_rate(learning_rate, epoch):
    max_lr = learning_rate*c.MAX_LR_FACTOR
    cycle = np.floor(1+(epoch/(2*c.LR_STEP_SIZE)))
    x = np.abs((epoch/c.LR_STEP_SIZE)-(2*cycle)+1)
    return learning_rate+(max_lr-learning_rate)*np.maximum(0,(1-x))
```
```python
c.DECAY_RATE = learning rate decay rate
c.MAX_LR_FACTOR = multiplier that determines the max learning rate
c.LR_STEP_SIZE = the number of epochs each cycle lasts
```
<br>With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.

<center><img src='./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png' width="540">
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br>
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

After **24 hours!**, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.
<a name='Model3'></a>

### Model 3 - new network topology

<center><img src='./assets/img/posts/20210318/Loss_function_and_Illegal_moves7.png' width="540">
<small>100,000 episodes, 635,000 epochs with batches of 64 moves each<br>
<b>Wins: 76.83%</b> Losses: 17.35% Ties: 5.82%</small></center>

<center><img src='./assets/img/posts/20210318/Game_Screen2.png' width="240" height="240">
<small>*I can still beat the network most of the time! (I am playing with the red X)*</small></center>

<center><img src='./assets/img/posts/20210318/Loss_function_and_Illegal_moves10.png' width="540">
<small>100,000 more episodes, 620,000 epochs with batches of 64 moves each<br>
<b>Wins: 82.25%</b> Losses: 13.28% Ties: 4.46%</small></center>

**Finally we crossed the 80% mark!** This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.

After more training rounds and some experimenting with the learning rate and other parameters, I couldn't improve past the 82.25% win rate.

These have been the results so far:

<center><img src='./assets/img/posts/20210318/Models1to3.png' width="540"></center>
<br>

It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with:
- the learning rate
- the network topology and activation functions
- the cycling and decaying learning rate parameters
- the batch size
- the target update cycle (when the target network is updated with the weights from the policy network)
- the rewards policy
- the epsilon greedy strategy
- whether to train vs. a random player or an "intelligent" AI.

And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.

<tweet>Network topology seems to have the biggest impact on a neural network's learning ability.</tweet>

<a name='Model4'></a>
### Model 4 - implementing momentum

I [reached out to the reddit community](https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/) and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with:

- Stochastic Gradient Descent with Momentum
- RMSProp: Root Mean Square Plain Momentum
- NAG: Nezterov's Accelerated Momentum
- Adam: Adaptive Moment Estimation
- and keep my old vanilla Gradient Descent (vGD) ☺

<a name='optimization'></a>[Click here for a detailed explanation and code of all the implemented optimization algorithms.](https://the-mvm.github.io/neural-network-optimization-methods/)

So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.
<a name='Model5'></a>
### Model 5 - implementing one-hot encoding and changing topology (again)
I came across an [interesting project in Github](https://github.com/AxiomaticUncertainty/Deep-Q-Learning-for-Tic-Tac-Toe/blob/master/tic_tac_toe.py) that deals exactly with Deep Q Learning, and I noticed that he used "one-hot" encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:

<center><img src='./assets/img/posts/20210318/Neural_Network_Topology3.png' width="540"></center>

So, 'one hot' encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn't easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.

Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.

To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.

<a name='Model6'></a>
### Model 6 - Tensorflow / Keras
<center><img src='https://www.kubeflow.org/docs/images/logos/TensorFlow.png' width="100" height="100"></center>

```python
self.PolicyNetwork = Sequential()
for layer in hidden_layers:
    self.PolicyNetwork.add(Dense(
                           units=layer,
                           activation='relu',
                           input_dim=inputs,
                           kernel_initializer='random_uniform',
                           bias_initializer='zeros'))
self.PolicyNetwork.add(Dense(
                        outputs,
                        kernel_initializer='random_uniform',
                        bias_initializer='zeros'))
opt = Adam(learning_rate=c.LEARNING_RATE,
           beta_1=c.GAMMA_OPT,
           beta_2=c.BETA,
           epsilon=c.EPSILON,
           amsgrad=False)
self.PolicyNetwork.compile(optimizer='adam',
                           loss='mean_squared_error',
                           metrics=['accuracy'])
```
As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.

The training function changed to:
```python
reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss',
                                         factor=0.1,
                                         patience=25)
history = self.PolicyNetwork.fit(np.asarray(states_to_train),
                                 np.asarray(targets_to_train),
                                 epochs=c.EPOCHS,
                                 batch_size=c.BATCH_SIZE,
                                 verbose=1,
                                 callbacks=[reduce_lr_on_plateau],
                                 shuffle=True)
```

With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn't change a thing on the training of the network, so the results kept being the same, **the loss function was still stagnating! My code was not the issue.**
<a name='Model7'></a>
### Model 7 - changing the training schedule
Next I tried to change the way the network was training as per [u/elBarto015](https://www.reddit.com/user/elBarto015) [advised me on reddit](https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&utm_medium=web2x&context=3).

The way I was training initially was:
- Games begin being simulated and the outcome recorded in the replay memory
- Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size.
- The games continue to be played between the random player and the network.
- Every move from either player generates a new training round, again with a random sample from the replay memory.
- This continues until the number of games set up conclude.

<center><img src='./assets/img/posts/20210318/ReplayMemoryBefore.png' width="540"></center>

The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.

The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.

This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.

<center><img src='./assets/img/posts/20210318/ReplayMemoryAfter.png' width="540"></center><br>

After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:

<center><img src='./assets/img/posts/20210318/Model7HyperParameters.png' width="540"><br>
<img src='./assets/img/posts/20210318/Model7.png' width="480">
</center><br>

As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about [self play](https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717), and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.

I feel the end is near... should I continue to update this post as new events unfold or shall I make it a multi post thread?
